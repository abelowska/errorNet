{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Extraction of features for different models of error-related brain activity and anxiety dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import ast\n",
    "import os.path as op\n",
    "import pickle\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.integrate import simpson\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from autoreject import AutoReject\n",
    "\n",
    "from transformers import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Load data\n",
    "\n",
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses_100_600_sonata/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.3, random_state=0)\n",
    "    \n",
    "    print(f\"train size: {len(h_train)} ; test size: {len(h_test)}\")\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*GNG-(\\d+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 5 or len(correct) < 5:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, participant_epochs, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename, dtype={'Demo_kod': object})\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"Demo_kod\"] + info, dtype={'Demo_kod': object})\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"Demo_kod\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )      \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [participant_epochs]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    # event_dict = {\n",
    "    #     \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "    #     \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "    #     \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "    #     \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "    #     \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "    #     \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "    #     \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "    #     \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    # }\n",
    "    \n",
    "    event_dict = {\n",
    "        'Stimulus/RE*ex*1_n*1_c_1*R*FB': 10003,\n",
    "        'Stimulus/RE*ex*1_n*1_c_1*R*FG': 10004,\n",
    "        'Stimulus/RE*ex*1_n*1_c_2*R': 10005,\n",
    "        'Stimulus/RE*ex*1_n*2_c_1*R': 10006,\n",
    "        'Stimulus/RE*ex*2_n*1_c_1*R': 10007,\n",
    "        'Stimulus/RE*ex*2_n*2_c_1*R*FB': 10008,\n",
    "        'Stimulus/RE*ex*2_n*2_c_1*R*FG': 10009,\n",
    "        'Stimulus/RE*ex*2_n*2_c_2*R': 10010,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10003, 10004, 10008, 10009],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10005, 10006, 10007, 10010],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = False\n",
    "    \n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "    \n",
    "    ar = AutoReject(random_state=random_state, n_jobs=10, verbose=0)\n",
    "    epochs_ar, reject_log = ar.fit_transform(epochs, return_log=True)\n",
    "\n",
    "    # reject trials bad at Fz\n",
    "    idx = reject_log.ch_names.index('Fz')\n",
    "    mask = (reject_log.labels[:,idx] == 1)\n",
    "    epochs_ar.drop(mask)\n",
    "    \n",
    "    print(f'Reject {len(np.where(mask == True)[0])} trials bad at Fz')    \n",
    "    \n",
    "    return epochs_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dc68c-de05-4885-b579-2d887884ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_names(data_df):\n",
    "    columns_dict = {\n",
    "        \"16-Rumination Full Scale\": \"RRQ\", # mean\n",
    "        \"05-DASS-21 Anxiety scale\": \"DASS-21 Anx\", # mean\n",
    "        \"05-DASS-21 Stress scale\": \"DASS-21 Stress\", # mean\n",
    "        \"05-DASS-21 Depression scale\": \"DASS-21 Dep\", # mean\n",
    "        \"04-STAI Trait MEAN\": \"STAI-T_M\", # mean\n",
    "        \"04-STAI STATE 1-2 DIFFERENCE\": \"STAI-S Diff\", # sum\n",
    "        \"number_error\" : \"uninhibited response\", # sum\n",
    "        \"number_inhibited\" : \"inhibited response\", # sum\n",
    "        \"04-STAI Trait SUM\": \"STAI-T\", # sum\n",
    "        \"07-BIS\": \"BIS\", # mean\n",
    "        \"14-Obsessive-Compulsive WASHING\": \"WASH\", # mean\n",
    "        \"14-Obsessive-Compulsive OBSESSING\": \"OBSESS\", # mean\n",
    "        \"14-Obsessive-Compulsive HOARDING\": \"HOARD\", # mean\n",
    "        \"14-Obsessive-Compulsive ORDERING\": \"ORD\", # mean\n",
    "        \"14-Obsessive-Compulsive CHECKING\": \"CHECK\", # mean\n",
    "        \"14-Obsessive-Compulsive NEUTRALIZING\": \"NEU\", # mean\n",
    "        \"18-Thought Suppression Inventory\": \"WBSI\", # mean\n",
    "        \"28-Intolerance of Uncertainty - Prospective Anxiety\": \"IUS-P\", # mean\n",
    "        \"28-Intolerance of Uncertainty - Inhibitory Anxiety\": \"IUS-I\", # mean\n",
    "        \"06-Self-Esteem Scale_SES Rosenberga\": \"SES\", # mean\n",
    "        \"07-BAS Dzialanie\": 'BAS_D', # mean\n",
    "        \"07-BAS Poszukiwanie przyjemnosci\": 'BAS_PRZY', # mean\n",
    "        \"07-BAS Wrazliwosc na nagrode\": 'BAS_NAG', # mean\n",
    "        \"22-Nonforgiveness - Full Scale\": 'NONFOR',\n",
    "        \"27-Indecisiveness Scale_Frost\": 'INDEC_F', # mean\n",
    "        \"03-SP (Punishment Sensitivity)\": 'PUN', # sum\n",
    "        \"03-SR (Reward Sensitivity)\": 'REW', # sum\n",
    "        \"15-Obsessional Beliefs - Inflated responsibility for harm\": 'HARM', # mean\n",
    "        \"15-Obsessional Beliefs - Importance/Control of Thoughts\": 'T-CTR', # mean\n",
    "        \"15-Obsessional Beliefs - Overestimation of threat\": \"OT\", # mean\n",
    "        \"15-Obsessional Beliefs - Perfectionism/ Intolerance of uncertainty\": 'OB_PERF', # mean\n",
    "        \"17-Perfectionism CMDA\": 'CMDA', # mean\n",
    "        \"17-Perfect PS-Personal Standards (7 items mean)\" : 'PS', # mean\n",
    "        \"19-Guilt sensitivity\": 'G_SE', # mean\n",
    "        \"31-NFC Nietolerancja wieloznaczności-FULL\": 'AMB', # mean\n",
    "        \"31-NFC Preferowanie przewidywalności-FULL\": 'PRED', # mean\n",
    "        \"32-High standards from Maximization Full Scale\" : 'STAND',   # mean\n",
    "        \"Wiek\": \"Age\",\n",
    "        \"Płeć\": \"Sex\",\n",
    "        \"Ręczność\": \"Handness\",    \n",
    "\n",
    "        #######\n",
    "        \"Rumination Full Scale\": \"RRQ\",\n",
    "        \"DASS-21 Anxiety scale 0-SUM\": \"DASS-21 Anx\", # sum\n",
    "        \"DASS-21 Stress scale 0-SUM\": \"DASS-21 Stress\", # sum\n",
    "        \"DASS-21 Depression scale 0-SUM\": \"DASS-21 Dep\", # sum\n",
    "        \"number_error\": \"uninhibited response\", # sum\n",
    "        \"number_inhibited\":  \"inhibited response\", # sum\n",
    "        \"STAI STATE 1-2 DIFFERENCE\": \"STAI-S Diff\", # sum\n",
    "        \"STAI Trait SUM\": \"STAI-T\", # sum\n",
    "        \"BIS\": \"BIS\", # mean\n",
    "        \"Obsessive-Compulsive WASHING\": \"WASH\", # mean\n",
    "        \"Obsessive-Compulsive OBSESSING\": \"OBSESS\", # mean\n",
    "        \"Obsessive-Compulsive HOARDING\": \"HOARD\", # mean\n",
    "        \"Obsessive-Compulsive ORDERING\": \"ORD\", # mean\n",
    "        \"Obsessive-Compulsive CHECKING\": \"CHECK\", # mean\n",
    "        \"Obsessive-Compulsive NEUTRALIZING\": \"NEU\", # mean\n",
    "        \"Thought Suppression Inventory\": \"WBSI\", # mean\n",
    "        \"Intolerance of Uncertainty - Prospective Anxiety\": \"IUS-P\", # mean\n",
    "        \"Intolerance of Uncertainty - Inhibitory Anxiety\": \"IUS-I\", # mean\n",
    "        \"Self-Esteem Scale_SES Rosenberga MEAN\": \"SES\", # mean\n",
    "        \"BAS Dzialanie\": 'BAS_D', # mean # drive\n",
    "        \"BAS Poszukiwanie przyjemnosci\": 'BAS_PRZY', # mean # fun seeking\n",
    "        \"BAS Wrazliwosc na nagrode\": 'BAS_NAG', # mean # responsivness\n",
    "        \"Indecisiveness Scale_Frost\": 'INDEC_F', # mean\n",
    "        \"SP (Punishment Sensitivity)\": 'PUN', # sum\n",
    "        \"SR (Reward Sensitivity)\": 'REW', # sum\n",
    "        \"Obsessional Beliefs - Inflated responsibility for harm\": 'HARM', # mean\n",
    "        \"Obsessional Beliefs - Importance/Control of Thoughts\": 'T-CTR', # mean\n",
    "        \"Obsessional Beliefs - Overestimation of threat\": \"OT\", # mean\n",
    "        \"Obsessional Beliefs - Perfectionism/ Intolerance of uncertainty\": 'OB_PERF', # mean\n",
    "        \"Perfect PS-Personal Standards (7 items mean)\" : 'PS', # mean\n",
    "        \"Guilt sensitivity\": 'G_SE', # mean\n",
    "        \"Nietolerancja wieloznaczności-FULL\": 'AMB', # mean\n",
    "        \"Preferowanie przewidywalności-FULL\": 'PRED', # mean\n",
    "        \"High standards from Maximization Full Scale\" : 'STAND',   # mean\n",
    "    }\n",
    "\n",
    "    data_df = data_df.rename(columns=columns_dict)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6b99d-7821-44cb-b669-8ce682ce193c",
   "metadata": {},
   "source": [
    "- read Study 1 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_3_5\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/all_scales_with_rt.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_train_opus_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_train_opus_df.name = df_name\n",
    "    epochs_train_opus_df = change_column_names(epochs_train_opus_df)\n",
    "    print(\"Done\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_train_opus_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_train_opus_df.name = df_name\n",
    "    epochs_train_opus_df = change_column_names(epochs_train_opus_df)\n",
    "    \n",
    "    # save loaded data into a pickle file\n",
    "    epochs_train_opus_df.to_pickle(\"../data/\" + epochs_train_opus_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa25d09-4ebb-4c3e-a89a-8ae1b80c7a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_opus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87e8aa-5ca4-4735-a004-7fd9a5a414d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_opus_df['STAI-T'] = epochs_train_opus_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffccca-5b70-4843-b39f-c90e38ce92a8",
   "metadata": {},
   "source": [
    "- read Study 2 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6b138-bc55-49c5-9eff-940a01b5bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_sonata_3_5_stai\"\n",
    "# df_name = \"GNG_reject_auto_3-5\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/Sonata_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_train_sonata_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_train_sonata_df.name = df_name\n",
    "    epochs_train_sonata_df = change_column_names(epochs_train_sonata_df)\n",
    "    print(\"Done\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_train_sonata_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_train_sonata_df.name = df_name\n",
    "    epochs_train_sonata_df = change_column_names(epochs_train_sonata_df)\n",
    "    \n",
    "    # save loaded data into a pickle file\n",
    "    epochs_train_sonata_df.to_pickle(\"../data/\" + epochs_train_sonata_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4ef30-2d15-43d6-966e-a4118714233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_sonata_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200393a-2f77-480d-b973-c92c6c64fa1a",
   "metadata": {},
   "source": [
    "Refine some data from questionnaries to reconcile data from two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2568a-e1d2-4c8b-92f7-151b862e1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_sonata_df.loc[epochs_train_sonata_df['Sex'] == 'Osoba niebinarna', 'Sex'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41a7a0-7541-4020-87ae-7f305d7a157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize DASS-21 subscales scores to mean\n",
    "epochs_train_sonata_df['DASS-21 Stress'] = epochs_train_sonata_df['DASS-21 Stress'].apply(lambda x: (x + 7)/7)\n",
    "epochs_train_sonata_df['DASS-21 Anx'] = epochs_train_sonata_df['DASS-21 Anx'].apply(lambda x: (x + 7)/7)\n",
    "epochs_train_sonata_df['DASS-21 Dep'] = epochs_train_sonata_df['DASS-21 Dep'].apply(lambda x: (x + 7)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219ac12-4837-41a1-abad-29ae5fdd4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize STAI state scale to mean (divide by amount of items in subscale)\n",
    "epochs_train_sonata_df['STAI-T'] = epochs_train_sonata_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b04671-603a-4678-8889-8f77cf9a28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value in STAI-S Diff score\n",
    "epochs_train_sonata_df.loc[epochs_train_sonata_df['STAI-S Diff'] == 'None', 'STAI-S Diff'] = None\n",
    "epochs_train_sonata_df['STAI-S Diff'] = np.array(epochs_train_sonata_df['STAI-S Diff'].to_list()).astype(np.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78171035-125b-4a1b-8b00-a8a8a7cfc594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs_train_sonata_df['STAI-S Diff'] = epochs_train_sonata_df['STAI-S Diff'].fillna(epochs_train_sonata_df['STAI-S Diff'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055815fa-0d47-4bdd-9caf-51d60ced5d91",
   "metadata": {},
   "source": [
    "#### Read data for external testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7bdc7-6af8-4c64-957b-33f71c7d5d6e",
   "metadata": {},
   "source": [
    "- read Study 1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c990ac1-14fc-4d0c-bf5d-d96f076e1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_3_5_test_performance\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/all_scales_with_rt.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_opus_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_test_opus_df.name = df_name\n",
    "    epochs_test_opus_df = change_column_names(epochs_test_opus_df)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_opus_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_opus_df.name = df_name\n",
    "    epochs_test_opus_df = change_column_names(epochs_test_opus_df)\n",
    "    \n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_opus_df.to_pickle(\"../data/\" + epochs_test_opus_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271cd307-5952-47a6-87f9-2a482da8924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_opus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b6d20-9599-4d6f-bf32-f8b98c14e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_opus_df['STAI-T'] = epochs_test_opus_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b716470-7b11-4423-ac6d-cc65f2446aa3",
   "metadata": {},
   "source": [
    "- read Study 2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54d80f-6c7f-4c9c-a4a2-4564dd15c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_sonata_3_5_test_stai\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/Sonata_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_sonata_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_test_sonata_df.name = df_name\n",
    "    epochs_test_sonata_df = change_column_names(epochs_test_sonata_df)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_sonata_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_sonata_df.name = df_name\n",
    "    epochs_test_sonata_df = change_column_names(epochs_test_sonata_df)\n",
    "    \n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_sonata_df.to_pickle(\"../data/\" + epochs_test_sonata_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ede1f-481c-4e12-9613-db9eeb0c147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_sonata_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea677c-4f7e-4ec9-b034-823aaa526172",
   "metadata": {},
   "source": [
    "Refine some data from questionnaries to reconcile data from two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09c36d-6613-4093-b6aa-4af0c49b3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_sonata_df.loc[epochs_test_sonata_df['Sex'] == 'Osoba niebinarna', 'Sex'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e3cdb-5907-48e4-99f3-89d2580780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize DASS-21 subscales scores to mean\n",
    "epochs_test_sonata_df['DASS-21 Stress'] = epochs_test_sonata_df['DASS-21 Stress'].apply(lambda x: (x + 7)/7)\n",
    "epochs_test_sonata_df['DASS-21 Anx'] = epochs_test_sonata_df['DASS-21 Anx'].apply(lambda x: (x + 7)/7)\n",
    "epochs_test_sonata_df['DASS-21 Dep'] = epochs_test_sonata_df['DASS-21 Dep'].apply(lambda x: (x + 7)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba725e66-37f2-4b5d-97bc-c509ab15468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize STAI state scale to mean (divide by amount of items in subscale)\n",
    "epochs_test_sonata_df['STAI-T'] = epochs_test_sonata_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b5fa8-e82a-4df8-b9ae-8acaf64a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value in STAI-S Diff score\n",
    "epochs_test_sonata_df.loc[epochs_test_sonata_df['STAI-S Diff'] == 'None', 'STAI-S Diff'] = None\n",
    "epochs_test_sonata_df['STAI-S Diff'] = np.array(epochs_test_sonata_df['STAI-S Diff'].to_list()).astype(np.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715e371-377a-465f-aefa-84cb7b8565cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs_test_sonata_df['STAI-S Diff'] = epochs_test_sonata_df['STAI-S Diff'].fillna(epochs_test_sonata_df['STAI-S Diff'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e49fc-28b8-47ef-8a5d-5dd1dcfd915e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b4bd7-f3b6-4707-8662-b266913fb3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_columns_list = epochs_train_opus_df.columns.to_list()\n",
    "sonata_columns_list = epochs_train_sonata_df.columns.to_list()\n",
    "\n",
    "columns = list(set(opus_columns_list) & set(sonata_columns_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cec6de-6ce0-458f-ad00-79de41daa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_sonata_df = epochs_train_sonata_df[columns]\n",
    "epochs_train_opus_df = epochs_train_opus_df[columns]\n",
    "\n",
    "epochs_test_sonata_df = epochs_test_sonata_df[columns]\n",
    "epochs_test_opus_df = epochs_test_opus_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01eda1-84ea-4034-834a-7d56bfd8e828",
   "metadata": {},
   "source": [
    "Create train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96e09d-e718-4a2e-9555-41c942673904",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_df = pd.concat([epochs_train_sonata_df, epochs_train_opus_df], ignore_index=True)\n",
    "epochs_test_df = pd.concat([epochs_test_sonata_df, epochs_test_opus_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2f7d8-49ac-4955-b8ff-9af12f4a3dd3",
   "metadata": {},
   "source": [
    "#### Chose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b99eb8-aa51-447d-81f2-cc7018f304df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False\n",
    "dataset = 'test' if test else 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b940ac-d7f7-44fd-bbda-6a9503a388fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df = epochs_train_df if not test else epochs_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98fe0f-a0ae-4022-93d8-b2382a82692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdb66d-6873-42d3-9290-11c006ba28ea",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe123fa-f506-44b7-ac74-d825cdc4ce06",
   "metadata": {},
   "source": [
    "- average number of responses in the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a99ef1-7836-4575-8c1b-ea6f608fe3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_len = epochs_train_df['epoch'].map(lambda x: len(x['error_response'].get_data())).to_numpy()\n",
    "correct_len = epochs_train_df['epoch'].map(lambda x: len(x['correct_response'].get_data())).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db209c-4a54-4380-b7f3-fbae72dd2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG number of incorrect responses in the train set: {np.mean(error_len)} (SD={np.std(error_len)})\")\n",
    "print(f\"AVG number of correct responses in the train set: {np.mean(correct_len)} (SD={np.std(correct_len)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686ab88-4d55-4603-b443-3489c97b2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_len = epochs_test_df['epoch'].map(lambda x: len(x['error_response'].get_data())).to_numpy()\n",
    "correct_len = epochs_test_df['epoch'].map(lambda x: len(x['correct_response'].get_data())).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bb09c-b5a6-4651-a11a-3892dc672f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG number of incorrect responses in the test set: {np.mean(error_len)} (SD={np.std(error_len)})\")\n",
    "print(f\"AVG number of correct responses in the test set: {np.mean(correct_len)} (SD={np.std(correct_len)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0ccfa-fc23-431b-9096-2e98fcfc3fa8",
   "metadata": {},
   "source": [
    "- difference in the 0-100 ms window between conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c761f0-a62e-44c3-a2fe-f8cb817ba807",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\"Fz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ded9f8-00f3-4593-b5e7-272e38d30759",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_train_df.to_dict()))\n",
    "\n",
    "ern_pipeline = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked()),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_ern_train = ern_pipeline.transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f9e30-65bb-469a-bc03-830e33b422ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_train_df.to_dict()))\n",
    "\n",
    "crn_pipeline = Pipeline([\n",
    "   (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked(condition='correct_response')),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_crn_train = crn_pipeline.transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85e02c-e2d7-4989-b712-77878915a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_test_df.to_dict()))\n",
    "\n",
    "ern_pipeline = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked()),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_ern_test = ern_pipeline.transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03cec2-ed16-43fd-abbc-9d77dff3a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_test_df.to_dict()))\n",
    "\n",
    "crn_pipeline = Pipeline([\n",
    "   (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked(condition='correct_response')),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_crn_test = crn_pipeline.transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bf50c-5cf4-4695-a1b2-6f44e5215a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_rel(preprocessed_X_ern_train, preprocessed_X_crn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931d91a-8614-4129-905a-303d82f11e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_rel(preprocessed_X_ern_test, preprocessed_X_crn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703671a-3adc-4286-8bf2-eb3d9dbd70bc",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d0fd1-f37a-494c-b39f-ebb93f639a11",
   "metadata": {},
   "source": [
    "#### Extract EEG features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9cc4c-083f-41a8-823e-b295966c91a4",
   "metadata": {},
   "source": [
    "1. Amplitude\n",
    "\n",
    "- ROI: Fz\n",
    "- time window: 0 - 100 ms\n",
    "- mean amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49871a-3b17-4a14-902a-a4bb6f9768ef",
   "metadata": {},
   "source": [
    "- ERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bab73-107f-4fb6-88fd-23c0818c23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\"Fz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632787dc-ac9c-45f9-8a96-d4cf8946bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))\n",
    "\n",
    "ern_pipeline = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked()),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_ern = ern_pipeline.transform(epochs_df_copy)\n",
    "preprocessed_X_ern = preprocessed_X_ern.reshape(preprocessed_X_ern.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9a858-21a4-4ec3-896d-f2dcc58aa24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X_ern.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809644a-3897-4998-9dea-43400edfdcf4",
   "metadata": {},
   "source": [
    "- CRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9fd22-ce0b-47e3-8e3e-b9c0b9bee2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\"Fz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107e50f-6552-4a38-87cc-f388ab629c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))\n",
    "\n",
    "crn_pipeline = Pipeline([\n",
    "   (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked(condition='correct_response')),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_crn = crn_pipeline.transform(epochs_df_copy)\n",
    "preprocessed_X_crn = preprocessed_X_crn.reshape(preprocessed_X_crn.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8315dd8-afa1-4011-924b-c8309a106c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X_crn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e0934-882f-4c13-bbba-5aed64e040bd",
   "metadata": {},
   "source": [
    "2. Latency: fractional area latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e0bd2-5952-4cab-9b60-f5fcd1720a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_negative_area_latency(evoked, fraction=0.5, tmin=0.0, tmax=0.5, threshold = 0.0):\n",
    "    subject_data = evoked.get_data()\n",
    "    x = np.linspace(tmin, tmax, subject_data.shape[-1])\n",
    "    y = subject_data.flatten()\n",
    "    \n",
    "    # get only negative part of signal\n",
    "    y_negative = [abs(y_item) if y_item < threshold else 0 for y_item in y]\n",
    "    \n",
    "    # calculate area under the signal\n",
    "    area = abs(simpson(y_negative, x))\n",
    "    \n",
    "    if area != 0.0:\n",
    "        fractional_area = area * fraction\n",
    "    \n",
    "        # search for latency point (x) which split area according to fraction provided \n",
    "        current_area = 0\n",
    "        fractional_area_index = 0\n",
    "        i = 2\n",
    "        while abs(simpson(y_negative[:i], x[:i])) <= fractional_area:\n",
    "            current_area = abs(simpson(y_negative[:i], x[:i]))\n",
    "            fractional_area_index = i\n",
    "            i+=1\n",
    "        return (fractional_area_index, x[fractional_area_index])    \n",
    "    else:\n",
    "        print('No area detected')\n",
    "        return (None, None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c08aa2-c473-4cf2-99ae-ae060aa71f78",
   "metadata": {},
   "source": [
    "- ERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a2ab3-e09a-4465-88ea-ee18f38c7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = ['Fz']\n",
    "tmin = -0.05\n",
    "tmax = 0.2\n",
    "threshold = 1*1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f1b4e-ebd9-488f-9fb6-0a052bc2b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b876351-bd1e-4312-8eda-57238b7d4e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=tmin, tmax=tmax)),\n",
    "    (\"average\", Evoked(condition='error_response')),\n",
    "]).fit_transform(epochs_df_copy)\n",
    "\n",
    "X = X[['evoked']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3d05b-8df2-4a1a-9a30-2957a3031ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_latencies = []\n",
    "for i in range(0, len(X)):        \n",
    "    subject = X[i][0]\n",
    "    evoked = subject\n",
    "    this_latency = fractional_negative_area_latency(evoked, fraction=0.5, tmin=tmin, tmax=tmax, threshold=threshold)\n",
    "    fractional_latencies.append(this_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6c738-7572-478a-ad64-16c95cb52076",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractional_latencies_ern = list(map(lambda x: x[1] ,fractional_latencies))\n",
    "fractional_latencies_ern = np.array(fractional_latencies_ern).reshape(-1,1)\n",
    "fractional_latencies_ern.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452666fd-0fc3-4cf7-b6e1-afdb8f1f6dd9",
   "metadata": {},
   "source": [
    "- CRN: threshold at $2 \\mu V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0b916-767b-4dee-8d15-f3d454bc268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = ['Fz']\n",
    "tmin = -0.05\n",
    "tmax = 0.2\n",
    "threshold = 2*1e-6 # threshold at 2 uV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa211f9-1fec-4009-aed7-a511ba36575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808b290-0145-437f-860a-ed28d7582371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=tmin, tmax=tmax)),\n",
    "    (\"average\", Evoked(condition='correct_response')),\n",
    "]).fit_transform(epochs_df_copy)\n",
    "\n",
    "X = X[['evoked']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e99ebf-31af-4b80-ba4f-90638af325b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_latencies = []\n",
    "for i in range(0, len(X)):        \n",
    "    subject = X[i][0]\n",
    "    evoked = subject\n",
    "    # print(f\"Index: {i}\")\n",
    "    this_latency = fractional_negative_area_latency(evoked, fraction=0.5, tmin=tmin, tmax=tmax, threshold=threshold)\n",
    "    fractional_latencies.append(this_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ebf18a-72d3-4826-8c9f-cd3034009ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractional_latencies_crn_2uV = list(map(lambda x: x[1] ,fractional_latencies))\n",
    "fractional_latencies_crn_2uV = np.array(fractional_latencies_crn_2uV).reshape(-1,1)\n",
    "fractional_latencies_crn_2uV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e6233-81b8-4f1f-8763-896f5f8beace",
   "metadata": {},
   "source": [
    "#### Extract questionnaires scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ff716-45b0-45f7-a06d-65e42168d78b",
   "metadata": {},
   "source": [
    "Define questionnaires to include in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32f83c-fd82-4a88-abd3-5d43f14ce256",
   "metadata": {},
   "outputs": [],
   "source": [
    "rumination = \"RRQ\"\n",
    "dass_stress = \"DASS-21 Stress\"\n",
    "dass_dep = \"DASS-21 Dep\"\n",
    "stai_t = \"STAI-T\" \n",
    "stai_s_diff = \"STAI-S Diff\" \n",
    "uninhibited_responses = \"uninhibited response\"\n",
    "inhibited_responses = \"inhibited response\"\n",
    "bis = \"BIS\"\n",
    "bas_dzialanie = \"BAS_D\"\n",
    "bas_przyjemnosc = \"BAS_PRZY\"\n",
    "bas_nagroda = \"BAS_NAG\"\n",
    "washing = \"WASH\"\n",
    "obsessing = \"OBSESS\"\n",
    "hoarding = \"HOARD\"\n",
    "ordering = \"ORD\"\n",
    "checking = \"CHECK\"\n",
    "neutralizing = \"NEU\"\n",
    "oci_r_full = \"OCI-R\"\n",
    "threat = \"OT\"\n",
    "thought_suppression = \"WBSI\"\n",
    "indecisivness = \"INDEC_F\"\n",
    "IU_prospecitve = \"IUS-P\"\n",
    "IU_inhibitory = \"IUS-I\"\n",
    "self_esteem = \"SES\"\n",
    "punishment_sensitivity = \"PUN\"\n",
    "reward_sensitivity = \"REW\"\n",
    "harm_responsibility = \"HARM\"\n",
    "thought_control = \"T-CTR\"\n",
    "perfectionism_IU = \"OB_PERF\"\n",
    "perfectionism_ps = \"PS\"\n",
    "guilt_sensitivity = \"G_SE\"\n",
    "intolerance_ambiguity = \"AMB\"\n",
    "predictability = \"PRED\"\n",
    "high_standards = \"STAND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb80bb5-46bd-4aba-b5d9-8402f97ea6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [\n",
    "    rumination,\n",
    "    dass_stress,\n",
    "    dass_dep,\n",
    "    stai_t,\n",
    "    stai_s_diff,\n",
    "    uninhibited_responses,\n",
    "    inhibited_responses,\n",
    "    bis,\n",
    "    bas_dzialanie,\n",
    "    bas_przyjemnosc,\n",
    "    bas_nagroda,\n",
    "    washing,\n",
    "    obsessing,\n",
    "    hoarding,\n",
    "    ordering,\n",
    "    checking,\n",
    "    neutralizing,\n",
    "    threat,\n",
    "    thought_suppression,\n",
    "    indecisivness,\n",
    "    punishment_sensitivity,\n",
    "    reward_sensitivity,\n",
    "    harm_responsibility,\n",
    "    guilt_sensitivity,\n",
    "    thought_control,\n",
    "    perfectionism_IU,\n",
    "    perfectionism_ps,\n",
    "    intolerance_ambiguity,\n",
    "    predictability,\n",
    "    high_standards,\n",
    "    IU_prospecitve,\n",
    "    IU_inhibitory,\n",
    "    self_esteem,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05652d55-e054-4c74-a22a-5bba52a28948",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df = epochs_df[scales]\n",
    "questionnaires_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3ec4b-64b2-40c1-993b-7bef3cd303b6",
   "metadata": {},
   "source": [
    "Fill missing value from external file - TODO to automatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7dfc9-86e7-4800-b84b-26714905a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df[questionnaires_scores_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e685f-3b39-497b-aec3-740c4a038091",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    questionnaires_scores_df.at[102, 'uninhibited response'] = 14.0\n",
    "    questionnaires_scores_df.at[102, 'inhibited response'] = 98.0\n",
    "else:\n",
    "    print('None to fill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd794d-1bf0-4651-9b93-617cacb9ab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df[questionnaires_scores_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623ec8b-725a-4067-898a-2017058e90aa",
   "metadata": {},
   "source": [
    "Create performance metric based on inhibited and uninhibited responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea34c0d-14ca-457e-9983-86e1e7542b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df['performance'] = questionnaires_scores_df['inhibited response'] / questionnaires_scores_df['uninhibited response']\n",
    "questionnaires_scores_df = questionnaires_scores_df.drop(columns=['inhibited response', 'uninhibited response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ba3c1-4846-4156-8b71-f92f0ae7815a",
   "metadata": {},
   "source": [
    "Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee36f5e-20eb-47ea-ac01-442a7405d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df[questionnaires_scores_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1a613-7dc1-4247-85ff-92df4e0d082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, 'display.max_columns', None):\n",
    "    display(questionnaires_scores_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58ee5f-ad0f-4149-bb3d-888cc2f2d357",
   "metadata": {},
   "source": [
    "Check skewness and kurtiosis\n",
    "\n",
    "As a general rule of thumb: If skewness is less than -1 or greater than 1, the distribution is highly skewed. If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed. If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n",
    "\n",
    "Analogous to the skewness, the general guideline is that if the kurtosis is greater than +2, the distribution is too peaked. Likewise, a kurtosis of less than −2 indicates a distribution that is too flat. When both skewness and kurtosis are close to zero, the pattern of responses is considered a normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053c7b3-cdde-4bd2-bd4a-832aba90247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = questionnaires_scores_df.agg(['skew', 'kurtosis', 'mean', 'std', 'min', 'max']).transpose()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec5df3-a3d5-4428-861c-d9b29c70729b",
   "metadata": {},
   "source": [
    "Transform entries with high skewness and kurtosis with BoxCox method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a07bab-4d38-45ec-875f-0238d5c7b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df_transformed = questionnaires_scores_df.copy()\n",
    "\n",
    "for row in summary.iterrows():\n",
    "    item_name = row[0]\n",
    "    skewness_ = row[1]['skew']\n",
    "    kurtosis_ = row[1]['kurtosis']\n",
    "    \n",
    "    if abs(skewness_) > 1 or abs(kurtosis_) > 2:\n",
    "        print(f'Transforming: {item_name} scale')\n",
    "        this_scores = questionnaires_scores_df_transformed[[item_name]].to_numpy().flatten()\n",
    "        this_scores = this_scores - np.min(this_scores) + 0.01\n",
    "        this_scores_transformed, lambda_ = boxcox(this_scores)\n",
    "        print(f'   Stats before transformation:\\n    Skewness: {scipy.stats.skew(this_scores)} \\n    Kurtosis: {scipy.stats.kurtosis(this_scores)}')\n",
    "        print(f'   Stats after transformation:\\n    Skewness: {scipy.stats.skew(this_scores_transformed)} \\n    Kurtosis: {scipy.stats.kurtosis(this_scores_transformed)}')\n",
    "        \n",
    "        questionnaires_scores_df_transformed[[item_name]] = this_scores_transformed.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca294a-0514-4a54-a57a-76aa6859a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = questionnaires_scores_df_transformed.agg(['skew', 'kurtosis', 'mean', 'std', 'min', 'max']).transpose()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72802dd9-ef2b-46cd-8a9d-08ce93c72229",
   "metadata": {},
   "source": [
    "#### Demographical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953a83d-5539-4f9d-a130-72c1b5d2456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = \"Age\"\n",
    "sex = \"Sex\"\n",
    "handness = \"Handness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2abf7e5-a77b-443d-a8a2-667a83b5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [\n",
    "    age,\n",
    "    sex,\n",
    "    handness\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ef9d2-28d1-4e2b-8ce9-18ef3adea3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographical_scores_df =  epochs_df[scales].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5708a5-ce72-43c3-a0cc-16f29d2638a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = demographical_scores_df.agg(['skew', 'kurtosis', 'mean', 'std', 'min', 'max']).transpose()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab4540-8d71-4736-ac33-c50a0428c0cc",
   "metadata": {},
   "source": [
    "Transform skewed age entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ad0b9-bc15-47b9-817e-fc466c906f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = demographical_scores_df[['Age']].to_numpy().flatten()\n",
    "age_transformed, lambda_ = boxcox(age)\n",
    "demographical_scores_df[['Age']] = age_transformed.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626ce17-e5c7-49d6-a790-82c1cd6b271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = demographical_scores_df.agg(['skew', 'kurtosis', 'mean', 'std', 'min', 'max']).transpose()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf8cbe-77f8-4ff1-94e2-85108eae85cc",
   "metadata": {},
   "source": [
    "#### Concatenate questionnaire and EEG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417570f-a028-4df8-afe5-f1b27eeee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_columns_ern = ['e_ERN']\n",
    "eeg_columns_crn = ['e_CRN']\n",
    "\n",
    "eeg_column_latencies_fal_2_crn = ['e_LT_F2_C']\n",
    "eeg_column_latencies_peak_crn = ['e_LT_P_C']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bb57d-3781-4238-a702-ae5ce9ea2a88",
   "metadata": {},
   "source": [
    "1. ERN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a406-dad4-4e48-ad07-dafca5789770",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ern_df = pd.DataFrame()\n",
    "\n",
    "results_ern_df[eeg_columns_ern] = preprocessed_X_ern\n",
    "results_ern_df = pd.concat([results_ern_df, questionnaires_scores_df_transformed.drop(columns=['performance'])], axis=1)\n",
    "\n",
    "results_ern_df.to_pickle(f\"../data/models_pickles_new_dass/ern_models_{dataset}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7ea02-ae5e-4f45-89b7-a278b631ad55",
   "metadata": {},
   "source": [
    "2. CRN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc00e6d-580a-4eeb-89c9-2b8573976554",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_crn_df = pd.DataFrame()\n",
    "\n",
    "results_crn_df[eeg_columns_crn] = preprocessed_X_crn\n",
    "results_crn_df = pd.concat([results_crn_df, questionnaires_scores_df_transformed.drop(columns=['performance'])], axis=1)\n",
    "\n",
    "results_crn_df.to_pickle(f\"../data/models_pickles_new_dass/crn_models_{dataset}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9e322-46a5-468a-af1e-0254feeaf3ea",
   "metadata": {},
   "source": [
    "3. ERN and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f33769-958e-41e6-a3d6-0270db6d44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ern_lat_df = pd.DataFrame()\n",
    "\n",
    "results_ern_lat_df[eeg_columns_ern] = preprocessed_X_ern\n",
    "results_ern_lat_df[eeg_column_latencies_fal] = fractional_latencies_ern \n",
    "results_ern_lat_df = pd.concat([results_ern_lat_df, demographical_scores_df, questionnaires_scores_df_transformed], axis=1)\n",
    "\n",
    "results_ern_lat_df.to_pickle(f\"../data/models_pickles_new_dass/ern_cov_fal_models_{dataset}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4b59d-c4f6-4981-8c24-e15469a1a3d0",
   "metadata": {},
   "source": [
    "4. CRN and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba4903-e223-45ed-9abf-cdccc5920eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_crn_lat_demo_df = pd.DataFrame()\n",
    "\n",
    "results_crn_lat_demo_df[eeg_columns_crn] = preprocessed_X_crn\n",
    "results_crn_lat_demo_df[eeg_column_latencies_fal_2_crn] = fractional_latencies_crn_2uV\n",
    "results_crn_lat_demo_df = pd.concat([results_crn_lat_demo_df, demographical_scores_df, questionnaires_scores_df_transformed], axis=1)\n",
    "\n",
    "results_crn_lat_demo_df.to_pickle(f\"../data/models_pickles_new_dass/crn_cov_fal2_models_{dataset}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44f9a2-034e-42e4-994a-57ace6006b67",
   "metadata": {},
   "source": [
    "## Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aec8d6-cff7-4e42-be58-163d35bdd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'test'\n",
    "epochs_df = epochs_train_df if dataset == 'train' else epochs_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78305c3-9926-435f-98db-1d5bff4b7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_data_df = pd.read_pickle(f\"../data/models_pickles_new_dass/ern_models_{dataset}.pkl\")\n",
    "ern_cov_fal_data_df = pd.read_pickle(f\"../data/models_pickles_new_dass/ern_cov_fal_models_{dataset}.pkl\")\n",
    "\n",
    "crn_data_df = pd.read_pickle(f\"../data/models_pickles_new_dass/crn_models_{dataset}.pkl\")\n",
    "crn_cov_fal2_data_df = pd.read_pickle(f\"../data/models_pickles_new_dass/crn_cov_fal2_models_{dataset}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d469a2-9878-479c-8f0f-48442d0ea051",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"RRQ\": \"Rumination\",\n",
    "    \"DASS-21 Stress\": \"Stress\",\n",
    "    \"DASS-21 Dep\": \"Depression\",\n",
    "    \"STAI-T\": \"Anxiety trait\",\n",
    "    \"STAI-S Diff\": 'Affective load',\n",
    "    \"BIS\": \"Behavioral inhibition\",\n",
    "    \"OBSESS\": \"Obsessing\",\n",
    "    \"HOARD\": \"Hoarding\",\n",
    "    \"ORD\": \"Ordering\",\n",
    "    \"CHECK\": \"Checking\",\n",
    "    'WASH': \"Washing\",\n",
    "    'NEU': \"Neutralizing\",\n",
    "    \"WBSI\": \"Thought supression\",\n",
    "    \"IUS-P\": \"Prospective IU\",\n",
    "    \"IUS-I\": \"Inhibitiory IU\",\n",
    "    \"SES\": \"Self-esteem\",\n",
    "    'BAS_D': \"Drive BAS\",\n",
    "    'BAS_PRZY': \"Fun-seeking BAS\",\n",
    "    'BAS_NAG': \"Reward responsiveness BAS \",\n",
    "    'INDEC_F': \"Indecisiveness\",\n",
    "    'PUN': \"Punishment sensitivity\",\n",
    "    'REW': \"Reward sensitivity\",\n",
    "    'HARM': \"Inflated harm responsibility\",\n",
    "    'T-CTR': \"Importance of thought control\",\n",
    "    \"OT\": \"Threat overestimation\",\n",
    "    'OB_PERF': \"Perfectionism/IU\",\n",
    "    'PS': \"Personal standandards\",\n",
    "    'G_SE': \"Guilt sensitivity\",\n",
    "    'AMB': \"Avoidance of ambiguity\",\n",
    "    'PRED': \"Need for predictability\",\n",
    "    'STAND': \"High standards\",   \n",
    "    \"Age\": \"Age\",\n",
    "    \"Handness\": \"Handedness\",\n",
    "    'e_ERN': \"ERN amplitude\",\n",
    "    'e_LT_F': \"ERN latency\",\n",
    "    'performance': \"Performance\",\n",
    "    'e_CRN': \"CRN amplitude\",\n",
    "    'e_LT_F2_C': \"CRN latency\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e985e78-7ca4-45fd-b4e7-9bf2d52fe404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.merge(ern_cov_fal_data_df, crn_cov_fal2_data_df, how='inner')\n",
    "data_df = data_df.rename(columns=mapping)\n",
    "\n",
    "data_df = pd.concat([epochs_df[['Demo_kod']], data_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c63fc1-c41b-4309-9f45-17eb68259041",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66696d4-fab1-43cd-8729-5caf3ea64278",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(f'../public_data/data_{dataset}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "net",
   "language": "python",
   "name": "net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
