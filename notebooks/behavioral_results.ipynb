{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Behavioral data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import ast\n",
    "import os.path as op\n",
    "import pickle\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "import pygsp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.integrate import simpson\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from autoreject import AutoReject\n",
    "\n",
    "from transformers import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Load data\n",
    "\n",
    "Loading EEG data and data from questionnaires. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths TODO\n",
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses_100_600_sonata/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.3, random_state=0)\n",
    "    \n",
    "    print(f\"train size: {len(h_train)} ; test size: {len(h_test)}\")\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*GNG-(\\d+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 5 or len(correct) < 5:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, participant_epochs, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename, dtype={'Demo_kod': object})\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"Demo_kod\"] + info, dtype={'Demo_kod': object})\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"Demo_kod\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )      \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [participant_epochs]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    # event_dict = {\n",
    "    #     \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "    #     \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "    #     \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "    #     \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "    #     \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "    #     \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "    #     \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "    #     \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    # }\n",
    "    \n",
    "    event_dict = {\n",
    "        'Stimulus/RE*ex*1_n*1_c_1*R*FB': 10003,\n",
    "        'Stimulus/RE*ex*1_n*1_c_1*R*FG': 10004,\n",
    "        'Stimulus/RE*ex*1_n*1_c_2*R': 10005,\n",
    "        'Stimulus/RE*ex*1_n*2_c_1*R': 10006,\n",
    "        'Stimulus/RE*ex*2_n*1_c_1*R': 10007,\n",
    "        'Stimulus/RE*ex*2_n*2_c_1*R*FB': 10008,\n",
    "        'Stimulus/RE*ex*2_n*2_c_1*R*FG': 10009,\n",
    "        'Stimulus/RE*ex*2_n*2_c_2*R': 10010,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10003, 10004, 10008, 10009],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10005, 10006, 10007, 10010],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = False\n",
    "    \n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "    \n",
    "    ar = AutoReject(random_state=random_state, n_jobs=10, verbose=0)\n",
    "    epochs_ar, reject_log = ar.fit_transform(epochs, return_log=True)\n",
    "    \n",
    "    return epochs_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dc68c-de05-4885-b579-2d887884ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_names(data_df):\n",
    "    columns_dict = {\n",
    "        \"Demo_kod\": \"ID\",\n",
    "        \"16-Rumination Full Scale\": \"RRQ\", # mean\n",
    "        \"05-DASS-21 Anxiety scale\": \"DASS-21 Anx\", # mean\n",
    "        ###\n",
    "        \"05-DASS-21 Stress scale\": \"DASS-21 Stress\", # mean\n",
    "        \"05-DASS-21 Depression scale\": \"DASS-21 Dep\", # mean\n",
    "        \"04-STAI Trait MEAN\": \"STAI-T_M\", # mean\n",
    "        \"04-STAI STATE 1-2 DIFFERENCE\": \"STAI-S Diff\", # sum\n",
    "        \"number_error\" : \"uninhibited response\", # sum\n",
    "        \"number_inhibited\" : \"inhibited response\", # sum\n",
    "        ###\n",
    "        \"04-STAI Trait SUM\": \"STAI-T\", # sum\n",
    "        \"07-BIS\": \"BIS\", # mean\n",
    "        \"14-Obsessive-Compulsive WASHING\": \"WASH\", # mean\n",
    "        \"14-Obsessive-Compulsive OBSESSING\": \"OBSESS\", # mean\n",
    "        \"14-Obsessive-Compulsive HOARDING\": \"HOARD\", # mean\n",
    "        \"14-Obsessive-Compulsive ORDERING\": \"ORD\", # mean\n",
    "        \"14-Obsessive-Compulsive CHECKING\": \"CHECK\", # mean\n",
    "        \"14-Obsessive-Compulsive NEUTRALIZING\": \"NEU\", # mean\n",
    "        # \"14-Obsessive-Compulsive FULL\": \"OCI-R\",\n",
    "        \"18-Thought Suppression Inventory\": \"WBSI\", # mean\n",
    "        \"28-Intolerance of Uncertainty - Prospective Anxiety\": \"IUS-P\", # mean\n",
    "        \"28-Intolerance of Uncertainty - Inhibitory Anxiety\": \"IUS-I\", # mean\n",
    "        \"06-Self-Esteem Scale_SES Rosenberga\": \"SES\", # mean\n",
    "        \"07-BAS Dzialanie\": 'BAS_D', # mean\n",
    "        \"07-BAS Poszukiwanie przyjemnosci\": 'BAS_PRZY', # mean\n",
    "        \"07-BAS Wrazliwosc na nagrode\": 'BAS_NAG', # mean\n",
    "        \"22-Nonforgiveness - Full Scale\": 'NONFOR',\n",
    "        \"27-Indecisiveness Scale_Frost\": 'INDEC_F', # mean\n",
    "        \"03-SP (Punishment Sensitivity)\": 'PUN', # sum\n",
    "        \"03-SR (Reward Sensitivity)\": 'REW', # sum\n",
    "        \"15-Obsessional Beliefs - Inflated responsibility for harm\": 'HARM', # mean\n",
    "        \"15-Obsessional Beliefs - Importance/Control of Thoughts\": 'T-CTR', # mean\n",
    "        \"15-Obsessional Beliefs - Overestimation of threat\": \"OT\", # mean\n",
    "        \"15-Obsessional Beliefs - Perfectionism/ Intolerance of uncertainty\": 'OB_PERF', # mean\n",
    "        \"17-Perfectionism CMDA\": 'CMDA', # mean\n",
    "        \"17-Perfect PS-Personal Standards (7 items mean)\" : 'PS', # mean\n",
    "        \"19-Guilt sensitivity\": 'G_SE', # mean\n",
    "        \"31-NFC Nietolerancja wieloznaczności-FULL\": 'AMB', # mean\n",
    "        \"31-NFC Preferowanie przewidywalności-FULL\": 'PRED', # mean\n",
    "        \"32-High standards from Maximization Full Scale\" : 'STAND',   # mean\n",
    "        \"Wiek\": \"Age\",\n",
    "        \"Płeć\": \"Sex\",\n",
    "        \"Ręczność\": \"Handness\",    \n",
    "\n",
    "        #######\n",
    "        \"Rumination Full Scale\": \"RRQ\",\n",
    "        \"DASS-21 Anxiety scale 0-SUM\": \"DASS-21 Anx\", # sum\n",
    "        \"DASS-21 Stress scale 0-SUM\": \"DASS-21 Stress\", # sum\n",
    "        \"DASS-21 Depression scale 0-SUM\": \"DASS-21 Dep\", # sum\n",
    "        \"number_error\": \"uninhibited response\", # sum\n",
    "        \"number_inhibited\":  \"inhibited response\", # sum\n",
    "        \"STAI STATE 1-2 DIFFERENCE\": \"STAI-S Diff\", # sum\n",
    "        ###\n",
    "        \"STAI Trait SUM\": \"STAI-T\", # sum\n",
    "        \"BIS\": \"BIS\", # mean\n",
    "        \"Obsessive-Compulsive WASHING\": \"WASH\", # mean\n",
    "        \"Obsessive-Compulsive OBSESSING\": \"OBSESS\", # mean\n",
    "        \"Obsessive-Compulsive HOARDING\": \"HOARD\", # mean\n",
    "        \"Obsessive-Compulsive ORDERING\": \"ORD\", # mean\n",
    "        \"Obsessive-Compulsive CHECKING\": \"CHECK\", # mean\n",
    "        \"Obsessive-Compulsive NEUTRALIZING\": \"NEU\", # mean\n",
    "        # \"14-Obsessive-Compulsive FULL\": \"OCI-R\",\n",
    "        \"Thought Suppression Inventory\": \"WBSI\", # mean\n",
    "        \"Intolerance of Uncertainty - Prospective Anxiety\": \"IUS-P\", # mean\n",
    "        \"Intolerance of Uncertainty - Inhibitory Anxiety\": \"IUS-I\", # mean\n",
    "        \"Self-Esteem Scale_SES Rosenberga MEAN\": \"SES\", # mean\n",
    "        \"BAS Dzialanie\": 'BAS_D', # mean # drive\n",
    "        \"BAS Poszukiwanie przyjemnosci\": 'BAS_PRZY', # mean # fun seeking\n",
    "        \"BAS Wrazliwosc na nagrode\": 'BAS_NAG', # mean # responsivness\n",
    "        # \"22-Nonforgiveness - Full Scale\": 'NONFOR',\n",
    "        \"Indecisiveness Scale_Frost\": 'INDEC_F', # mean\n",
    "        \"SP (Punishment Sensitivity)\": 'PUN', # sum\n",
    "        \"SR (Reward Sensitivity)\": 'REW', # sum\n",
    "        \"Obsessional Beliefs - Inflated responsibility for harm\": 'HARM', # mean\n",
    "        \"Obsessional Beliefs - Importance/Control of Thoughts\": 'T-CTR', # mean\n",
    "        \"Obsessional Beliefs - Overestimation of threat\": \"OT\", # mean\n",
    "        \"Obsessional Beliefs - Perfectionism/ Intolerance of uncertainty\": 'OB_PERF', # mean\n",
    "        # \"17-Perfectionism CMDA\": 'CMDA',\n",
    "        \"Perfect PS-Personal Standards (7 items mean)\" : 'PS', # mean\n",
    "        \"Guilt sensitivity\": 'G_SE', # mean\n",
    "        \"Nietolerancja wieloznaczności-FULL\": 'AMB', # mean\n",
    "        \"Preferowanie przewidywalności-FULL\": 'PRED', # mean\n",
    "        \"High standards from Maximization Full Scale\" : 'STAND',   # mean\n",
    "    }\n",
    "\n",
    "    data_df = data_df.rename(columns=columns_dict)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6b99d-7821-44cb-b669-8ce682ce193c",
   "metadata": {},
   "source": [
    "- read Study 1 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_3_5\"\n",
    "# df_name = \"GNG_reject_auto_3-5\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/Sonata_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_train_opus_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_train_opus_df.name = df_name\n",
    "    epochs_train_opus_df = change_column_names(epochs_train_opus_df)\n",
    "    print(\"Done\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_train_opus_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_train_opus_df.name = df_name\n",
    "    epochs_train_opus_df = change_column_names(epochs_train_opus_df)\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_train_opus_df.to_pickle(\"../data/\" + epochs_train_opus_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa25d09-4ebb-4c3e-a89a-8ae1b80c7a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_opus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87e8aa-5ca4-4735-a004-7fd9a5a414d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_opus_df['STAI-T'] = epochs_train_opus_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffccca-5b70-4843-b39f-c90e38ce92a8",
   "metadata": {},
   "source": [
    "- read Study 2 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6b138-bc55-49c5-9eff-940a01b5bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_sonata_3_5_stai\"\n",
    "# df_name = \"GNG_reject_auto_3-5\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/Sonata_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_train_sonata_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_train_sonata_df.name = df_name\n",
    "    epochs_train_sonata_df = change_column_names(epochs_train_sonata_df)\n",
    "    print(\"Done\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_train_sonata_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_train_sonata_df.name = df_name\n",
    "    epochs_train_sonata_df = change_column_names(epochs_train_sonata_df)\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_train_sonata_df.to_pickle(\"../data/\" + epochs_train_sonata_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4ef30-2d15-43d6-966e-a4118714233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_sonata_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200393a-2f77-480d-b973-c92c6c64fa1a",
   "metadata": {},
   "source": [
    "Refine some data from questionnaries to reconcile data from two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2568a-e1d2-4c8b-92f7-151b862e1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_sonata_df.loc[epochs_train_sonata_df['Sex'] == 'Osoba niebinarna', 'Sex'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41a7a0-7541-4020-87ae-7f305d7a157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize DASS-21 subscales scores to mean\n",
    "epochs_train_sonata_df['DASS-21 Stress'] = epochs_train_sonata_df['DASS-21 Stress'].apply(lambda x: (x + 7)/7)\n",
    "epochs_train_sonata_df['DASS-21 Anx'] = epochs_train_sonata_df['DASS-21 Anx'].apply(lambda x: (x + 7)/7)\n",
    "epochs_train_sonata_df['DASS-21 Dep'] = epochs_train_sonata_df['DASS-21 Dep'].apply(lambda x: (x + 7)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219ac12-4837-41a1-abad-29ae5fdd4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize STAI state scale to mean (divide by amount of items in subscale)\n",
    "epochs_train_sonata_df['STAI-T'] = epochs_train_sonata_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b04671-603a-4678-8889-8f77cf9a28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value in STAI-S Diff score\n",
    "epochs_train_sonata_df.loc[epochs_train_sonata_df['STAI-S Diff'] == 'None', 'STAI-S Diff'] = None\n",
    "epochs_train_sonata_df['STAI-S Diff'] = np.array(epochs_train_sonata_df['STAI-S Diff'].to_list()).astype(np.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78171035-125b-4a1b-8b00-a8a8a7cfc594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs_train_sonata_df['STAI-S Diff'] = epochs_train_sonata_df['STAI-S Diff'].fillna(epochs_train_sonata_df['STAI-S Diff'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055815fa-0d47-4bdd-9caf-51d60ced5d91",
   "metadata": {},
   "source": [
    "#### Read data for external testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7bdc7-6af8-4c64-957b-33f71c7d5d6e",
   "metadata": {},
   "source": [
    "- read Study 1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c990ac1-14fc-4d0c-bf5d-d96f076e1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_3_5_test_performance\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "# info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "# info_filename = \"../data/scales/all_scales.csv\"\n",
    "info_filename = \"../data/scales/Sonata_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_opus_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_test_opus_df.name = df_name\n",
    "    epochs_test_opus_df = change_column_names(epochs_test_opus_df)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_opus_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_opus_df.name = df_name\n",
    "    epochs_test_opus_df = change_column_names(epochs_test_opus_df)\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_opus_df.to_pickle(\"../data/\" + epochs_test_opus_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271cd307-5952-47a6-87f9-2a482da8924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_opus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b6d20-9599-4d6f-bf32-f8b98c14e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_opus_df['STAI-T'] = epochs_test_opus_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b716470-7b11-4423-ac6d-cc65f2446aa3",
   "metadata": {},
   "source": [
    "- read Study 2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54d80f-6c7f-4c9c-a4a2-4564dd15c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "df_name = \"GNG_autoreject_sonata_3_5_test_stai\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "# info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "# info_filename = \"../data/scales/all_scales.csv\"\n",
    "info_filename = \"../data/scales/Sonata_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_sonata_df = pd.read_pickle(pickled_data_filename)\n",
    "    epochs_test_sonata_df.name = df_name\n",
    "    epochs_test_sonata_df = change_column_names(epochs_test_sonata_df)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_sonata_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_sonata_df.name = df_name\n",
    "    epochs_test_sonata_df = change_column_names(epochs_test_sonata_df)\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_sonata_df.to_pickle(\"../data/\" + epochs_test_sonata_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ede1f-481c-4e12-9613-db9eeb0c147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_sonata_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea677c-4f7e-4ec9-b034-823aaa526172",
   "metadata": {},
   "source": [
    "Refine some data from questionnaries to reconcile data from two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09c36d-6613-4093-b6aa-4af0c49b3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_test_sonata_df.loc[epochs_test_sonata_df['Sex'] == 'Osoba niebinarna', 'Sex'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e3cdb-5907-48e4-99f3-89d2580780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize DASS-21 subscales scores to mean\n",
    "epochs_test_sonata_df['DASS-21 Stress'] = epochs_test_sonata_df['DASS-21 Stress'].apply(lambda x: (x + 7)/7)\n",
    "epochs_test_sonata_df['DASS-21 Anx'] = epochs_test_sonata_df['DASS-21 Anx'].apply(lambda x: (x + 7)/7)\n",
    "epochs_test_sonata_df['DASS-21 Dep'] = epochs_test_sonata_df['DASS-21 Dep'].apply(lambda x: (x + 7)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba725e66-37f2-4b5d-97bc-c509ab15468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize STAI state scale to mean (divide by amount of items in subscale)\n",
    "epochs_test_sonata_df['STAI-T'] = epochs_test_sonata_df['STAI-T'].apply(lambda x: x/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b5fa8-e82a-4df8-b9ae-8acaf64a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value in STAI-S Diff score\n",
    "epochs_test_sonata_df.loc[epochs_test_sonata_df['STAI-S Diff'] == 'None', 'STAI-S Diff'] = None\n",
    "epochs_test_sonata_df['STAI-S Diff'] = np.array(epochs_test_sonata_df['STAI-S Diff'].to_list()).astype(np.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715e371-377a-465f-aefa-84cb7b8565cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs_test_sonata_df['STAI-S Diff'] = epochs_test_sonata_df['STAI-S Diff'].fillna(epochs_test_sonata_df['STAI-S Diff'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e49fc-28b8-47ef-8a5d-5dd1dcfd915e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b4bd7-f3b6-4707-8662-b266913fb3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_columns_list = epochs_train_opus_df.columns.to_list()\n",
    "sonata_columns_list = epochs_train_sonata_df.columns.to_list()\n",
    "\n",
    "columns = list(set(opus_columns_list) & set(sonata_columns_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cec6de-6ce0-458f-ad00-79de41daa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_sonata_df = epochs_train_sonata_df[columns]\n",
    "epochs_train_opus_df = epochs_train_opus_df[columns]\n",
    "\n",
    "epochs_test_sonata_df = epochs_test_sonata_df[columns]\n",
    "epochs_test_opus_df = epochs_test_opus_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01eda1-84ea-4034-834a-7d56bfd8e828",
   "metadata": {},
   "source": [
    "Create train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96e09d-e718-4a2e-9555-41c942673904",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_df = pd.concat([epochs_train_sonata_df, epochs_train_opus_df], ignore_index=True)\n",
    "epochs_test_df = pd.concat([epochs_test_sonata_df, epochs_test_opus_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822db226-f29a-4425-8ab5-443c3d73bf97",
   "metadata": {},
   "source": [
    "## Basic sample info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166534ac-46dd-45a9-824c-8d255c7ab943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([epochs_train_df, epochs_test_df], ignore_index=True)\n",
    "data_df[\"Sex\"] = pd.to_numeric(data_df[\"Sex\"])\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38bbf1-17d8-4f1a-8191-0922ae1b6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"Sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdb66d-6873-42d3-9290-11c006ba28ea",
   "metadata": {},
   "source": [
    "## Behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7ae7b-8743-4b7b-a2a3-9024283e4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_opus_df = pd.read_csv(\"../data/behavioral/stats_opus.csv\", dtype={'ID': object})\n",
    "stats_sonata_df = pd.read_csv(\"../data/behavioral/stats_sonata.csv\", dtype={'ID': object})\n",
    "\n",
    "stats_df = pd.concat([stats_opus_df, stats_sonata_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5e55d-7c08-4a5d-8361-8f53670b69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_train_list = epochs_train_df.id.to_list()\n",
    "ID_test_list = epochs_test_df.id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04e379-5207-4fed-b942-65099a02483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = stats_df[stats_df['ID'].isin(ID_train_list)]\n",
    "test_df = stats_df[stats_df['ID'].isin(ID_test_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7ada3-adb2-4894-b092-c0f9d63dc6cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The train dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56d40-a99e-42c4-9f9c-77102725ec26",
   "metadata": {},
   "source": [
    "Extract RT of correct and error trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4180dc3-8dce-42f8-b1b0-65ce5c50558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rt = train_df[\"mean_error_RT\"].to_numpy()\n",
    "correct_rt = train_df[\"mean_hit_RT\"].to_numpy()\n",
    "error_trials_num = train_df[\"number_error\"].to_numpy()\n",
    "correct_trials_num = train_df[\"number_fast_hit\"].to_numpy() + train_df[\"number_slow_hit\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f79721-3ea5-42ce-af27-46c8eeb16f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X train average number of:\\n erroneous responses: {error_trials_num.mean()} SD = {error_trials_num.std()}\\n correct responses: {correct_trials_num.mean()} SD = {correct_trials_num.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6e5e7-271d-4923-9039-d88033300e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X train average RT for:\\n erroneous responses: {error_rt.mean()} SD = {error_rt.std()}\\n correct responses: {correct_rt.mean()} SD = {correct_rt.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e47b7-bb75-4c06-9459-b51428fe7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_value, p_value = scipy.stats.ttest_rel(error_rt, correct_rt)\n",
    "print(f\"t({train_df.shape[0] -1}) = {t_value}, p = {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ccb34-0b19-40a3-8b39-074f627b3e4a",
   "metadata": {},
   "source": [
    "Average number of trials included in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a99ef1-7836-4575-8c1b-ea6f608fe3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_len = epochs_train_df['epoch'].map(lambda x: len(x['error_response'].get_data())).to_numpy()\n",
    "correct_len = epochs_train_df['epoch'].map(lambda x: len(x['correct_response'].get_data())).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db209c-4a54-4380-b7f3-fbae72dd2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG number of incorrect responses in the train set: {np.mean(error_len)} (SD={np.std(error_len)})\")\n",
    "print(f\"AVG number of correct responses in the train set: {np.mean(correct_len)} (SD={np.std(correct_len)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d5353-42c0-4e87-ab8e-a651dc4b2dc2",
   "metadata": {},
   "source": [
    "Average performnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b520b-3f04-48f2-8f6e-d9fee80836c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['performance'] = train_df['number_inhibited'] / (train_df['number_inhibited'] + train_df['number_error'])\n",
    "performance = train_df['performance'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08530147-cc25-401a-a5da-284615843b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG performance in the test set: {np.mean(performance)} (SD={np.std(performance)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d31939-ff93-4abb-b6fe-e3057c44f676",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The test dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb334b-61e4-4df1-bb93-c22b3fa1276e",
   "metadata": {},
   "source": [
    "Extract RT of correct and error trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723e61b-a75f-468d-b4a6-529fb8f34437",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rt = test_df[\"mean_error_RT\"].to_numpy()\n",
    "correct_rt = test_df[\"mean_hit_RT\"].to_numpy()\n",
    "error_trials_num = test_df[\"number_error\"].to_numpy()\n",
    "correct_trials_num = test_df[\"number_fast_hit\"].to_numpy() + test_df[\"number_slow_hit\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01a0a7-e8c7-47b0-bafc-b4d6b0aca728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X test average number of:\\n erroneous responses: {error_trials_num.mean()} SD = {error_trials_num.std()}\\n correct responses: {correct_trials_num.mean()} SD = {correct_trials_num.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6fed7-31b1-4b21-8aef-ed93398ca20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X test average RT for:\\n erroneous responses: {error_rt.mean()} SD = {error_rt.std()}\\n correct responses: {correct_rt.mean()} SD = {correct_rt.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e59e96-4a05-46b6-8a61-952a0ad12d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_rel(error_rt, correct_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe123fa-f506-44b7-ac74-d825cdc4ce06",
   "metadata": {},
   "source": [
    "Average number of trials included in the analusis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686ab88-4d55-4603-b443-3489c97b2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_len = epochs_test_df['epoch'].map(lambda x: len(x['error_response'].get_data())).to_numpy()\n",
    "correct_len = epochs_test_df['epoch'].map(lambda x: len(x['correct_response'].get_data())).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bb09c-b5a6-4651-a11a-3892dc672f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG number of incorrect responses in the test set: {np.mean(error_len)} (SD={np.std(error_len)})\")\n",
    "print(f\"AVG number of correct responses in the test set: {np.mean(correct_len)} (SD={np.std(correct_len)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9462dc20-7f77-4457-847e-d3624693b18b",
   "metadata": {},
   "source": [
    "Average performnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7f0f0-7c7c-45f0-93bd-385965ed55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['performance'] = test_df['number_inhibited'] / (test_df['number_inhibited'] + test_df['number_error'])\n",
    "performance = test_df['performance'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dafb07f-4976-435e-a6b4-704725511833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG performance in the test set: {np.mean(performance)} (SD={np.std(performance)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f43e5f-74b6-4bc7-a633-504afbe94f6f",
   "metadata": {},
   "source": [
    "## EEG features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3911e8-18bc-43d9-93a9-13f1c906ecc4",
   "metadata": {},
   "source": [
    "### Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc145b9-3c72-473f-9ef3-6f8fe51701ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\n",
    "    \"Fz\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16d4af-1b86-43ad-8ec1-e87d49686fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_pipeline = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked()),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "])\n",
    "\n",
    "crn_pipeline = Pipeline([\n",
    "   (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked(condition='correct_response')),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607a787-37e2-41cc-89df-5e15ed11bc4e",
   "metadata": {},
   "source": [
    "- training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0ccfa-fc23-431b-9096-2e98fcfc3fa8",
   "metadata": {},
   "source": [
    "Difference in the 0-100 ms window between conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ded9f8-00f3-4593-b5e7-272e38d30759",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_train_df.to_dict()))\n",
    "\n",
    "preprocessed_X_ern_train = ern_pipeline.fit_transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f9e30-65bb-469a-bc03-830e33b422ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_train_df.to_dict()))\n",
    "\n",
    "preprocessed_X_crn_train = crn_pipeline.fit_transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed4c0d-4289-41a3-8729-76299fb60905",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG ERN amplitude in the test set: {np.mean(preprocessed_X_ern_train*1000000)} (SD={np.std(preprocessed_X_ern_train*1000000)})\")\n",
    "print(f\"AVG CRN amplitude in the test set: {np.mean(preprocessed_X_crn_train*1000000)} (SD={np.std(preprocessed_X_crn_train*1000000)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bf50c-5cf4-4695-a1b2-6f44e5215a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_rel(preprocessed_X_ern_train, preprocessed_X_crn_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613027e-65dd-47eb-a49c-3989925861ae",
   "metadata": {},
   "source": [
    "- testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee3a99-a3ed-40dd-acc1-968305b657fe",
   "metadata": {},
   "source": [
    "Difference in the 0-100 ms window between conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85e02c-e2d7-4989-b712-77878915a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_test_df.to_dict()))\n",
    "\n",
    "preprocessed_X_ern_test = ern_pipeline.fit_transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03cec2-ed16-43fd-abbc-9d77dff3a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_test_df.to_dict()))\n",
    "\n",
    "preprocessed_X_crn_test = crn_pipeline.fit_transform(epochs_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f2c2f-bad1-4d66-950d-3a30dcd48671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AVG ERN amplitude in the test set: {np.mean(preprocessed_X_ern_test*1000000)} (SD={np.std(preprocessed_X_ern_test*1000000)})\")\n",
    "print(f\"AVG CRN amplitude in the test set: {np.mean(preprocessed_X_crn_test*1000000)} (SD={np.std(preprocessed_X_crn_test*1000000)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931d91a-8614-4129-905a-303d82f11e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_rel(preprocessed_X_ern_test, preprocessed_X_crn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29c314-1b56-4a77-b3f2-bd1d9ee3ec52",
   "metadata": {},
   "source": [
    "#### Difference between training and testing sets in amplitudes of ERN and CRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b037cd-a3c2-4599-b5d9-497fada8febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(preprocessed_X_ern_train, preprocessed_X_ern_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cffd0d-be2c-45cd-91ee-bcebf57b31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(preprocessed_X_crn_train, preprocessed_X_crn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29854646-f265-43f2-a19f-1e1da2171d91",
   "metadata": {},
   "source": [
    "## Questionnaries, Covariates, and ERPs descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffeec0-4364-410b-836a-e426b290366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True\n",
    "dataset = 'test' if test else 'train'\n",
    "epochs_df = epochs_test_df if test else epochs_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd6ffe-c775-42a9-9a6c-bc7afe75c29e",
   "metadata": {},
   "source": [
    "ERN\n",
    "\n",
    "- ROI: Fz\n",
    "- time window: 0 - 100 ms\n",
    "- mean amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bab73-107f-4fb6-88fd-23c0818c23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\n",
    "    \"Fz\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632787dc-ac9c-45f9-8a96-d4cf8946bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))\n",
    "\n",
    "ern_pipeline = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked()),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_ern = ern_pipeline.transform(epochs_df_copy)\n",
    "preprocessed_X_ern = preprocessed_X_ern.reshape(preprocessed_X_ern.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9a858-21a4-4ec3-896d-f2dcc58aa24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X_ern.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b03190-1462-4b20-a797-5199809109e3",
   "metadata": {},
   "source": [
    "CRN\n",
    "\n",
    "- ROI: Fz\n",
    "- time window: 0 - 100 ms\n",
    "- mean amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9fd22-ce0b-47e3-8e3e-b9c0b9bee2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\n",
    "    \"Fz\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107e50f-6552-4a38-87cc-f388ab629c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))\n",
    "\n",
    "crn_pipeline = Pipeline([\n",
    "   (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "    (\"average\", Evoked(condition='correct_response')),\n",
    "    ('extract_averaged_data', ExtractData()),\n",
    "    (\"mean_amplitude\", AverageSignal()),\n",
    "    \n",
    "]).fit(epochs_df_copy)\n",
    "\n",
    "preprocessed_X_crn = crn_pipeline.transform(epochs_df_copy)\n",
    "preprocessed_X_crn = preprocessed_X_crn.reshape(preprocessed_X_crn.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8315dd8-afa1-4011-924b-c8309a106c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X_crn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c195d-efda-4d92-818f-2c9e1dd7743f",
   "metadata": {},
   "source": [
    "Fractional area latency - ERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e0bd2-5952-4cab-9b60-f5fcd1720a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_negative_area_latency(evoked, fraction=0.5, tmin=0.0, tmax=0.5, threshold = 0.0):\n",
    "    subject_data = evoked.get_data()\n",
    "    x = np.linspace(tmin, tmax, subject_data.shape[-1])\n",
    "    # print(x)\n",
    "    y = subject_data.flatten()\n",
    "    \n",
    "    # get only negative part of signal\n",
    "    y_negative = [abs(y_item) if y_item < threshold else 0 for y_item in y]\n",
    "    \n",
    "    # calculate area under the signal\n",
    "    area = abs(simpson(y_negative, x))\n",
    "    \n",
    "    if area != 0.0:\n",
    "        fractional_area = area * fraction\n",
    "    \n",
    "        # search for latency point (x) which split area according to fraction provided \n",
    "        current_area = 0\n",
    "        fractional_area_index = 0\n",
    "        i = 2\n",
    "        while abs(simpson(y_negative[:i], x[:i])) <= fractional_area:\n",
    "            current_area = abs(simpson(y_negative[:i], x[:i]))\n",
    "            fractional_area_index = i\n",
    "            i+=1\n",
    "            \n",
    "        # print(f'{fractional_area_index}; {x[fractional_area_index]}')\n",
    "        # print(x)\n",
    "        \n",
    "        return (fractional_area_index, x[fractional_area_index])    \n",
    "    else:\n",
    "        print('No area detected')\n",
    "        return (None, None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c08aa2-c473-4cf2-99ae-ae060aa71f78",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a2ab3-e09a-4465-88ea-ee18f38c7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = ['Fz']\n",
    "tmin = -0.05\n",
    "tmax = 0.2\n",
    "threshold = 1*1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f52e32-9503-4c1f-adfa-b58441b5d07e",
   "metadata": {},
   "source": [
    "Estimate fractional area latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f1b4e-ebd9-488f-9fb6-0a052bc2b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b876351-bd1e-4312-8eda-57238b7d4e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=tmin, tmax=tmax)),\n",
    "    (\"average\", Evoked(condition='error_response')),\n",
    "]).fit_transform(epochs_df_copy)\n",
    "\n",
    "X = X[['evoked']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3d05b-8df2-4a1a-9a30-2957a3031ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_latencies = []\n",
    "for i in range(0, len(X)):        \n",
    "    subject = X[i][0]\n",
    "    evoked = subject\n",
    "    # print(f\"Index: {i}\")\n",
    "    this_latency = fractional_negative_area_latency(evoked, fraction=0.5, tmin=tmin, tmax=tmax, threshold=threshold)\n",
    "    fractional_latencies.append(this_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6c738-7572-478a-ad64-16c95cb52076",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractional_latencies_ern = list(map(lambda x: x[1] ,fractional_latencies))\n",
    "fractional_latencies_ern = np.array(fractional_latencies_ern).reshape(-1,1)\n",
    "fractional_latencies_ern.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452666fd-0fc3-4cf7-b6e1-afdb8f1f6dd9",
   "metadata": {},
   "source": [
    "Fractional area latency - CRN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f2b64-1b66-4c43-8c07-d0b0556d97f4",
   "metadata": {},
   "source": [
    "Parameters: threshold at $2 \\mu V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0b916-767b-4dee-8d15-f3d454bc268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = ['Fz']\n",
    "tmin = -0.05\n",
    "tmax = 0.2\n",
    "threshold = 2*1e-6 # przy tym thresholdzie nie lapiemy wszystkich osob 1= 7 os; 2 = wszyscy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a6b29-28dd-4244-8ce4-eea2f724808a",
   "metadata": {},
   "source": [
    "Estimate fractional area latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa211f9-1fec-4009-aed7-a511ba36575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_copy = pd.DataFrame(copy.deepcopy(epochs_df.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808b290-0145-437f-860a-ed28d7582371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = Pipeline([\n",
    "    (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "    (\"trim\", EpochTrim(tmin=tmin, tmax=tmax)),\n",
    "    (\"average\", Evoked(condition='correct_response')),\n",
    "]).fit_transform(epochs_df_copy)\n",
    "\n",
    "X = X[['evoked']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e99ebf-31af-4b80-ba4f-90638af325b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_latencies = []\n",
    "for i in range(0, len(X)):        \n",
    "    subject = X[i][0]\n",
    "    evoked = subject\n",
    "    # print(f\"Index: {i}\")\n",
    "    this_latency = fractional_negative_area_latency(evoked, fraction=0.5, tmin=tmin, tmax=tmax, threshold=threshold)\n",
    "    fractional_latencies.append(this_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ebf18a-72d3-4826-8c9f-cd3034009ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractional_latencies_crn_2uV = list(map(lambda x: x[1] ,fractional_latencies))\n",
    "fractional_latencies_crn_2uV = np.array(fractional_latencies_crn_2uV).reshape(-1,1)\n",
    "fractional_latencies_crn_2uV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed840b5-1039-4f23-bdc3-426ce0034bda",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e6233-81b8-4f1f-8763-896f5f8beace",
   "metadata": {},
   "source": [
    "#### Extract anxiety-related questionnaires scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ff716-45b0-45f7-a06d-65e42168d78b",
   "metadata": {},
   "source": [
    "Questionnaires to include in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32f83c-fd82-4a88-abd3-5d43f14ce256",
   "metadata": {},
   "outputs": [],
   "source": [
    "rumination = \"RRQ\"\n",
    "dass_anxiety = \"DASS-21 Anx\"\n",
    "dass_stress = \"DASS-21 Stress\"\n",
    "dass_dep = \"DASS-21 Dep\"\n",
    "stai_t = \"STAI-T\" \n",
    "stai_s_diff = \"STAI-S Diff\" \n",
    "uninhibited_responses = \"uninhibited response\"\n",
    "inhibited_responses = \"inhibited response\"\n",
    "bis = \"BIS\"\n",
    "bas_dzialanie = \"BAS_D\"\n",
    "bas_przyjemnosc = \"BAS_PRZY\"\n",
    "bas_nagroda = \"BAS_NAG\"\n",
    "washing = \"WASH\"\n",
    "obsessing = \"OBSESS\"\n",
    "hoarding = \"HOARD\"\n",
    "ordering = \"ORD\"\n",
    "checking = \"CHECK\"\n",
    "neutralizing = \"NEU\"\n",
    "oci_r_full = \"OCI-R\"\n",
    "threat = \"OT\"\n",
    "thought_suppression = \"WBSI\"\n",
    "indecisivness = \"INDEC_F\"\n",
    "IU_prospecitve = \"IUS-P\"\n",
    "IU_inhibitory = \"IUS-I\"\n",
    "self_esteem = \"SES\"\n",
    "punishment_sensitivity = \"PUN\"\n",
    "reward_sensitivity = \"REW\"\n",
    "harm_responsibility = \"HARM\"\n",
    "thought_control = \"T-CTR\"\n",
    "perfectionism_IU = \"OB_PERF\"\n",
    "# perfectionism_cmda = \"17-Perfectionism CMDA\"\n",
    "perfectionism_ps = \"PS\"\n",
    "guilt_sensitivity = \"G_SE\"\n",
    "intolerance_ambiguity = \"AMB\"\n",
    "predictability = \"PRED\"\n",
    "high_standards = \"STAND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb80bb5-46bd-4aba-b5d9-8402f97ea6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [\n",
    "    rumination,\n",
    "    # dass_anxiety,\n",
    "    dass_stress,\n",
    "    dass_dep,\n",
    "    stai_t,\n",
    "    stai_s_diff,\n",
    "    uninhibited_responses,\n",
    "    inhibited_responses,\n",
    "    bis,\n",
    "    bas_dzialanie,\n",
    "    bas_przyjemnosc,\n",
    "    bas_nagroda,\n",
    "    washing,\n",
    "    obsessing,\n",
    "    hoarding,\n",
    "    ordering,\n",
    "    checking,\n",
    "    neutralizing,\n",
    "    # oci_r_full,\n",
    "    threat,\n",
    "    thought_suppression,\n",
    "    indecisivness,\n",
    "    punishment_sensitivity,\n",
    "    reward_sensitivity,\n",
    "    harm_responsibility,\n",
    "    guilt_sensitivity,\n",
    "    thought_control,\n",
    "    perfectionism_IU,\n",
    "    # perfectionism_cmda,\n",
    "    perfectionism_ps,\n",
    "    intolerance_ambiguity,\n",
    "    predictability,\n",
    "    high_standards,\n",
    "    IU_prospecitve,\n",
    "    IU_inhibitory,\n",
    "    self_esteem,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05652d55-e054-4c74-a22a-5bba52a28948",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df = epochs_df[scales]\n",
    "questionnaires_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3ec4b-64b2-40c1-993b-7bef3cd303b6",
   "metadata": {},
   "source": [
    "Fill missing value from external file - TODO to automatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ecb81-4c40-4214-b3b1-46da34ff7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df[questionnaires_scores_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701decca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    questionnaires_scores_df.at[102, 'uninhibited response'] = 14.0\n",
    "    questionnaires_scores_df.at[102, 'inhibited response'] = 98.0\n",
    "else:\n",
    "    print('None to fill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e794b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df[questionnaires_scores_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623ec8b-725a-4067-898a-2017058e90aa",
   "metadata": {},
   "source": [
    "Create performance metric based on inhibited and uninhibited responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3402e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df['performance'] = questionnaires_scores_df['inhibited response'] / questionnaires_scores_df['uninhibited response']\n",
    "questionnaires_scores_df = questionnaires_scores_df.drop(columns=['inhibited response', 'uninhibited response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ba3c1-4846-4156-8b71-f92f0ae7815a",
   "metadata": {},
   "source": [
    "Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires_scores_df[questionnaires_scores_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1a613-7dc1-4247-85ff-92df4e0d082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, 'display.max_columns', None):\n",
    "    display(questionnaires_scores_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72802dd9-ef2b-46cd-8a9d-08ce93c72229",
   "metadata": {},
   "source": [
    "#### Demographical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953a83d-5539-4f9d-a130-72c1b5d2456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = \"Age\"\n",
    "sex = \"Sex\"\n",
    "handness = \"Handness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2abf7e5-a77b-443d-a8a2-667a83b5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [\n",
    "    age,\n",
    "    # sex,\n",
    "    handness\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ef9d2-28d1-4e2b-8ce9-18ef3adea3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographical_scores_df =  epochs_df[scales].astype(float)\n",
    "demographical_scores_df = demographical_scores_df.rename(columns={'Handness': 'Handedness'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a88e0-a085-4a03-a5a7-c1e93a952794",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographical_scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b0101-ae54-4575-9a5d-412e3d61c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographical_scores_df['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf8cbe-77f8-4ff1-94e2-85108eae85cc",
   "metadata": {},
   "source": [
    "#### Concatenate questionnaire, covariates, and EEG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fea69-69a5-4205-84a3-f6ac5666cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    fractional_latencies_crn_2uV[fractional_latencies_crn_2uV == None] = np.nan\n",
    "    fractional_latencies_crn_2uV_df = pd.DataFrame(fractional_latencies_crn_2uV)\n",
    "    fractional_latencies_crn_2uV_df.fillna(value = np.nanmean(fractional_latencies_crn_2uV), inplace=True)\n",
    "    fractional_latencies_crn_2uV = fractional_latencies_crn_2uV_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a406-dad4-4e48-ad07-dafca5789770",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "eeg_features_df = pd.DataFrame({\n",
    "    'ERN' : preprocessed_X_ern.flatten() * 1000000,\n",
    "    'ERN lat' : fractional_latencies_ern.flatten() * 1000,\n",
    "    'CRN' : preprocessed_X_crn.flatten() * 1000000,\n",
    "    'CRN lat' : fractional_latencies_crn_2uV.flatten() * 1000,\n",
    "\n",
    "    \n",
    "})\n",
    "\n",
    "results_df = pd.concat([questionnaires_scores_df, demographical_scores_df, eeg_features_df], axis=1)\n",
    "# results_df.to_pickle(f\"../data/behavioral/all_variables_{dataset}.pkl\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c6b9a-d90d-4d76-92fe-b0f9245c33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_mapping = {\n",
    "    \"STAI-T\": \"STAI-T\",\n",
    "    \"STAI-S Diff\": 'STAI-S Diff',\n",
    "    \"DASS-21 Stress\": \"DASS-21 Stress\",\n",
    "    \"DASS-21 Dep\": \"DASS-21 Dep\",\n",
    "    \"RRQ\": \"RRQ\",\n",
    "    \"BIS\": \"BIS\",\n",
    "    'BAS_D': \"BAS Drive\",\n",
    "    'BAS_PRZY': \"BAS R-R\",\n",
    "    'BAS_NAG': \"BAS F-S\",\n",
    "    'PUN': \"Punishment\",\n",
    "    'REW': \"Reward\",\n",
    "    \"CHECK\": \"Checking\",\n",
    "    \"HOARD\": \"Hoarding\",\n",
    "    \"OBSESS\": \"Obsessing\",\n",
    "    \"ORD\": \"Ordering\",\n",
    "    'NEU': 'Neutralizing',\n",
    "    \"WASH\": \"Washing\",\n",
    "    'INDEC_F': \"Indecisivness\",\n",
    "    \"WBSI\": \"WBSI\",\n",
    "    \"IUS-P\": \"IUS-P\",\n",
    "    \"IUS-I\": \"IUS-I\",\n",
    "    'HARM': \"Harm-R\",\n",
    "    \"OT\": \"OT\",\n",
    "    'T-CTR': \"T-CTR\",\n",
    "    'OB_PERF': \"Perfectionism\",\n",
    "    'PS': \"PS\",\n",
    "    'G_SE': \"Guilt-S\",\n",
    "    'AMB': \"Ambiguity-A\",\n",
    "    'PRED': \"Predictability\",\n",
    "    'STAND': \"H-Standards\",  \n",
    "    \"SES\": \"SES\",\n",
    "    \"Age\": \"Age\",\n",
    "    \"Handness\": \"Handness\",\n",
    "    'performance': \"Performance\",\n",
    "    'ERN': \"ERN\",\n",
    "    'ERN lat': 'ERN latency',\n",
    "    'CRN': \"CRN\",\n",
    "    'CRN lat': 'CRN latency',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b612d-e7c2-4835-bb77-6c63c7ff94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, 'display.max_columns', None):\n",
    "    display(results_df.rename(columns=column_names_mapping).describe().round(decimals=2).transpose())\n",
    "    results_df.rename(columns=column_names_mapping).describe().round(decimals=2).transpose().to_csv(f'../data/behavioral/all_variables_{dataset}_describe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407be4c-c16b-4f1d-a6e3-d59934139e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_renamed = results_df.rename(columns=column_names_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee06c2f-fbe5-4adc-9d41-ab42e40979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_df_renamed.copy()\n",
    "\n",
    "df[\"DASS-21 Stress\"] = (df[\"DASS-21 Stress\"] - 1)/3\n",
    "df[\"DASS-21 Dep\"] = (df[\"DASS-21 Dep\"] - 1)/3\n",
    "df[\"STAI-T\"] = (df[\"STAI-T\"] -1 ) /3\n",
    "df[\"BIS\"] = (df[\"BIS\"]-1 ) /3\n",
    "df[\"BAS Drive\"] = (df[\"BAS Drive\"]-1 ) /3\n",
    "df[\"BAS R-R\"] = (df[\"BAS R-R\"]-1 ) /3\n",
    "df[\"BAS F-S\"] = (df[\"BAS F-S\"]-1 ) /3\n",
    "df[\"RRQ\"] = (df[\"RRQ\"]-1 ) /4\n",
    "df[\"WBSI\"] = (df[\"WBSI\"]-1 ) /4\n",
    "df[\"OT\"] = (df[\"OT\"] -1) /6\n",
    "df[\"IUS-P\"] = (df[\"IUS-P\"]-1 ) /4\n",
    "df[\"IUS-I\"] = (df[\"IUS-I\"]-1 ) /4\n",
    "df[\"Checking\"] = (df[\"Checking\"]-1 ) /4\n",
    "df[\"Hoarding\"] = (df[\"Hoarding\"]-1 ) /4\n",
    "df[\"Obsessing\"] = (df[\"Obsessing\"]-1 ) /4\n",
    "df[\"Ordering\"] = (df[\"Ordering\"]-1 ) /4\n",
    "df[\"Neutralizing\"] = (df[\"Neutralizing\"]-1 ) /4\n",
    "df[\"Washing\"] = (df[\"Washing\"]-1 ) /4\n",
    "df[\"SES\"] = (df[\"SES\"]-1 ) /3\n",
    "\n",
    "df['Punishment'] = df['Punishment'] / 21\n",
    "df['Reward'] = df['Reward'] / 21\n",
    "df['Indecisivness'] = (df['Indecisivness'] - 1) / 4\n",
    "df['Harm-R'] = (df['Harm-R'] - 1) /6\n",
    "df['T-CTR'] = (df['T-CTR'] - 1) /6\n",
    "df['Perfectionism'] = (df['Perfectionism'] -1 ) /6\n",
    "df['PS'] = (df['PS'] - 1) / 4\n",
    "df['Guilt-S'] = (df['Guilt-S'] - 1) /4\n",
    "df['Ambiguity-A'] = (df['Ambiguity-A'] - 1) /5\n",
    "df['Predictability'] = (df['Predictability'] - 1) /5\n",
    "df['H-Standards'] = (df['H-Standards'] - 1) /6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2939a25-0d3d-4c50-91c7-5b44d1e6be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, 'display.max_columns', None):\n",
    "    display(df.describe().round(decimals=2).transpose())\n",
    "    # df.describe().round(decimals=2).transpose().to_csv(f'../data/behavioral/all_variables_{dataset}_describe_normalized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afcf6a-65a5-4797-b50f-d8233dd7b4dc",
   "metadata": {},
   "source": [
    "## Calculate differences between testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b3ec9-326a-410c-be98-a4e9a5aa7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df # done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c694d-6e06-4e33-9b05-17ae94e61775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d0fec-58a8-471c-8101-1f7a28ba9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_training:\n",
    "    variable_training = df_training[column].to_numpy().astype(float)\n",
    "    variable_testing = df_testing[column].to_numpy().astype(float)\n",
    "    \n",
    "    stats, p_val = scipy.stats.ttest_ind(variable_training, variable_testing)\n",
    "    print(f'---: \\n  {column}     \\n  p-value: {p_val}')\n",
    "    if p_val < 0.05:\n",
    "        print('Different')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85d4ae-391b-40ef-a612-9e03f58d5399",
   "metadata": {},
   "source": [
    "## Distributions of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21dc09-a0d3-4981-9b2f-9f0e3f3f6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the transformed data\n",
    "transformed_data_training = []\n",
    "\n",
    "# Iterate through each row of the original DataFrame\n",
    "for index, row in df_training.iterrows():\n",
    "    # Iterate through each column (questionnaire scale)\n",
    "    for scale_name, score in row.items():\n",
    "        # Append a dictionary containing score and scale name to the transformed data list\n",
    "        transformed_data_training.append({'score': score, 'scale': scale_name, 'dataset': 'training'})\n",
    "\n",
    "# Create a new DataFrame from the transformed data list\n",
    "transformed_data_training_df = pd.DataFrame(transformed_data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498564cc-e636-459b-90c5-5136c0c3c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d2544-e6c4-41a1-ad52-3bc5107cb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the transformed data\n",
    "transformed_data_testing = []\n",
    "\n",
    "# Iterate through each row of the original DataFrame\n",
    "for index, row in df_testing.iterrows():\n",
    "    # Iterate through each column (questionnaire scale)\n",
    "    for scale_name, score in row.items():\n",
    "        # Append a dictionary containing score and scale name to the transformed data list\n",
    "        transformed_data_testing.append({'score': score, 'scale': scale_name, 'dataset': 'testing'})\n",
    "\n",
    "# Create a new DataFrame from the transformed data list\n",
    "transformed_data_testing_df = pd.DataFrame(transformed_data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23020689-9dda-4e35-922a-571da8d6ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ee37a-fa7d-46ed-a3d3-4d56059cde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df = pd.concat([transformed_data_training_df, transformed_data_testing_df], ignore_index=True)\n",
    "variables_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe217a-43e2-4dbc-bd15-cdf3ba215283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(variables_df_renamed['scale'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f2eb2-4fcb-44d5-b474-72cccdc4ebe9",
   "metadata": {},
   "source": [
    "Rename scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571bcafd-9eaa-4b02-bdd0-09d165a613dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_names_mapping = {\n",
    "    \"STAI-T\": \"Anxiety trait\",\n",
    "    \"STAI-S Diff\": 'Affective load',\n",
    "    \"DASS-21 Stress\": \"Stress\",\n",
    "    \"DASS-21 Dep\": \"Depression\",\n",
    "    \"RRQ\": \"Rumination\",\n",
    "    \"BIS\": \"Behavioral inhibition\",\n",
    "    'BAS Drive': \"Drive BAS\",\n",
    "    'BAS R-R': \"Reward responsiveness BAS\",\n",
    "    'BAS F-S': \"Fun-seeking BAS\",\n",
    "    'Punishment': \"Punishment sensitivity\",\n",
    "    'Reward': \"Reward sensitivity\",\n",
    "    \"Checking\": \"Checking\",\n",
    "    \"Hoarding\": \"Hoarding\",\n",
    "    \"Obsessing\": \"Obsessing\",\n",
    "    \"Ordering\": \"Ordering\",\n",
    "    'Neutralizing': 'Neutralizing',\n",
    "    \"Washing\": \"Washing\",\n",
    "    'Indecisivness': \"Indecisiveness\",\n",
    "    \"WBSI\": \"Thought supression\",\n",
    "    \"IUS-P\": \"Prospective IU\",\n",
    "    \"IUS-I\": \"Inhibitory IU\",\n",
    "    'Harm-R': \"Inflated harm responsibility\",\n",
    "    \"OT\": \"Threat overestimation\",\n",
    "    'T-CTR': \"Importance of thought control\",\n",
    "    'Perfectionism': \"Perfectionism/IU\",\n",
    "    'PS': \"Personal standards\",\n",
    "    'Guilt-S': \"Guilt sensitivity\",\n",
    "    \"Ambiguity-A\": \"Avoidance of ambiguity\",\n",
    "    'Predictability': \"Need for predictability\",\n",
    "    'H-Standards': \"High standards\",  \n",
    "    \"SES\": \"Self-esteem\",\n",
    "    \"Age\": \"Age\",\n",
    "    \"Handness\": \"Handness\",\n",
    "    'Performance': \"Performance\",\n",
    "    'ERN': \"ERN amplitude\",\n",
    "    'ERN latency': 'ERN latency',\n",
    "    'CRN': \"CRN amplitude\",\n",
    "    'CRN latency': 'CRN latency',\n",
    "}\n",
    "variables_df_renamed = variables_df.copy()\n",
    "variables_df_renamed['scale'] = variables_df_renamed['scale'].replace(scales_names_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba75eb-32fe-441f-bbc9-8d8fb2a9685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['Rumination', 'Stress', 'Depression', 'Anxiety trait', 'Affective load', 'Behavioral inhibition', 'Obsessing', 'Hoarding' , 'Ordering', 'Checking', 'Washing', \n",
    "         'Neutralizing', 'Thought supression', 'Prospective IU', 'Inhibitory IU', 'Self-esteem', 'Drive BAS', 'Fun-seeking BAS', 'Reward responsiveness BAS', 'Indecisiveness',\n",
    "        'Punishment sensitivity', 'Reward sensitivity', 'Inflated harm responsibility', 'Importance of thought control', 'Threat overestimation', 'Perfectionism/IU', \n",
    "        'Personal standards', 'Guilt sensitivity', 'Avoidance of ambiguity', 'Need for predictability', 'High standards', 'Age', 'Handedness', 'Performance', 'ERN latency', \n",
    "        'CRN latency', 'ERN amplitude', 'CRN amplitude']\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "cm = 1/2.54\n",
    "dpi = 500\n",
    "\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams[\"axes.edgecolor\"] = \".15\"\n",
    "plt.rcParams[\"axes.linewidth\"]  = 0.5\n",
    "plt.rcParams[\"axes.linewidth\"]  = 0.5\n",
    "\n",
    "colors = [sns.color_palette(\"colorblind\")[0], sns.color_palette(\"colorblind\")[1], 'gray']\n",
    "sns.set_palette(colors)\n",
    "\n",
    "# custom_palette = [\"#4dac26\",  '#d01c8b']\n",
    "# sns.set_palette(custom_palette)\n",
    "# sns.set_palette('colorblind')\n",
    "\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    variables_df_renamed, \n",
    "    col='scale', \n",
    "    col_wrap=5,\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    despine=False,\n",
    "    height=3.5*cm,\n",
    "    aspect= 1.5,\n",
    "    # subplot_kws={\"xlim\":(0,1)},\n",
    "    legend_out=False,\n",
    "    hue='dataset',\n",
    "    col_order=order,\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.histplot, \n",
    "    x=\"score\",\n",
    "    kde=True,\n",
    "    cbar_kws={'saturation':0.9}\n",
    ")\n",
    "\n",
    "g.add_legend()\n",
    "\n",
    "fig = g.fig\n",
    "fig.set_size_inches(19*cm, 30*cm)\n",
    "\n",
    "g.fig.subplots_adjust(wspace=.2, hspace=.55)\n",
    "g.set_ylabels(label=None, clear_inner=True)\n",
    "g.set_titles(template=\"{col_name}\")\n",
    "\n",
    "sns.move_legend(\n",
    "    g, \"lower center\",\n",
    "    bbox_to_anchor=(.5, -0.04), ncol=2, title=None, frameon=False,\n",
    ")\n",
    "plt.setp(g._legend.get_texts(), fontsize=9)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'../data/scales_density/train_test_scales_density', bbox_inches='tight', pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102ad8a-527b-4d14-950f-7911d5483d38",
   "metadata": {},
   "source": [
    "## Demographical data of the whole sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7dc97-2b27-4fd6-a0f0-123fc8d2bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_demo_df =  pd.read_csv('../data/scales/all_scales_with_rt.csv')\n",
    "sonata_demo_df = pd.read_csv('../data/scales/Sonata_scales.csv', dtype={'Demo_kod': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff440439-d9a5-472a-812c-0bfcf862041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_demo_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485b5de-8ed8-47c6-921e-77e5d9a66892",
   "metadata": {},
   "source": [
    "Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df56877-5835-443a-86e6-75f167f62773",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(opus_demo_df.groupby('Płeć').describe())\n",
    "display(sonata_demo_df.groupby('Płeć').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c33674-b3ac-455e-955f-1d3d67bd4756",
   "metadata": {},
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5313c-4b54-4490-b0fd-3fc33dd4ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df = pd.concat([opus_demo_df[['Wiek']], sonata_demo_df[['Wiek']]], ignore_index=True)\n",
    "display(age_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab2467-9dbd-4046-bd23-b398c49ffc20",
   "metadata": {},
   "source": [
    "Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd0ae5-cee1-4cbb-9a84-df78321f0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_df = pd.concat([opus_demo_df[['Dotychczasowa liczba lat edukacji']], sonata_demo_df[['Twoja dotychczasowa liczba lat edukacji (w pełnych latach)']]], ignore_index=True)\n",
    "display(edu_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666b796-170e-4d55-a082-954be8972de1",
   "metadata": {},
   "source": [
    "#### Opus rejected individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc97e56-2f79-4b1b-94cf-505849eda1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_q_ids = pd.read_csv('../data/scales/all_scales_with_rt.csv',dtype={'Demo_kod': object})['Demo_kod'].to_numpy()\n",
    "opus_q_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50282315-12e5-4a2a-b840-bb96c3a96c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = '../data/responses_100_600/'\n",
    "opus_eeg_ids = []\n",
    "\n",
    "for file in sorted(glob.glob(dir_ + \"*.vhdr\")):\n",
    "    re_match = re.search('GNG_(.*)-64', file)\n",
    "    id_ = re_match.groups()[0]\n",
    "    opus_eeg_ids.append(id_)\n",
    "opus_eeg_ids = np.array(opus_eeg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d6f44-0aa0-4824-afc4-3d536f8f95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_eeg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c083d-15d0-4d4f-944a-2cc179563d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(opus_q_ids).difference(opus_eeg_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a53b6-ae30-4c6d-aedb-1a857f26357a",
   "metadata": {},
   "source": [
    "#### Sonata rejected individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b29169-b20b-4cf6-84f2-be9e031c6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonata_q_ids = pd.read_csv('../data/scales/Sonata_scales.csv', dtype={'Demo_kod': object})['Demo_kod'].to_numpy()\n",
    "sonata_q_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae23af-5516-4a40-b945-8c3f5cac7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = '../data/responses_100_600_sonata/'\n",
    "sonata_eeg_ids = []\n",
    "\n",
    "for file in sorted(glob.glob(dir_ + \"*.vhdr\")):\n",
    "    re_match = re.search('.*-GNG-(.*)_', file)\n",
    "    id_ = re_match.groups()[0]\n",
    "    sonata_eeg_ids.append(id_)\n",
    "sonata_eeg_ids = np.array(sonata_eeg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edeccb-5e92-4b5a-8816-10c310a60bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonata_eeg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84964053-80d3-4c57-a25f-cdb20bae8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(sonata_q_ids).difference(sonata_eeg_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e174697-3459-4a6d-b4bf-8f47f9d74ed4",
   "metadata": {},
   "source": [
    "## Demo infos about loaded sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08d2f4-aa1c-4a00-aa21-1220a968263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ID_train_list + ID_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeada32c-238a-40e4-8932-0330e1ca11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_edu_df = opus_demo_df[['Dotychczasowa liczba lat edukacji', 'Demo_kod']].rename(columns={'Dotychczasowa liczba lat edukacji': 'Education'})\n",
    "sonata_edu_df = sonata_demo_df[['Twoja dotychczasowa liczba lat edukacji (w pełnych latach)', 'Demo_kod']].rename(columns={'Twoja dotychczasowa liczba lat edukacji (w pełnych latach)': 'Education'})\n",
    "\n",
    "edu_df = pd.concat([opus_edu_df, sonata_edu_df], ignore_index=True)\n",
    "edu_loaded_sample_df = edu_df[edu_df['Demo_kod'].isin(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8012766-be10-4a29-951b-57461f74d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_loaded_sample_df['Education'].iloc[47] = 13.5\n",
    "edu_loaded_sample_df['Education'] = pd.to_numeric(edu_loaded_sample_df['Education'])\n",
    "edu_loaded_sample_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aebbb94-2402-4984-a662-9f0de6c1ff20",
   "metadata": {},
   "source": [
    "## Calculate Cronbach's alpha for questionnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a955b87-0a34-43d1-b9a3-456d77f31c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = f\"../data/scales/net_scales/\"\n",
    "\n",
    "scales = []\n",
    "\n",
    "for file in sorted(glob.glob(dir_ + \"*.csv\")):\n",
    "    print(file)\n",
    "    scale_df = pd.read_csv(file)\n",
    "    scales.append(scale_df)\n",
    "\n",
    "    columns_to_drop = [col for col in scale_df.columns if ('kod' in col.lower()) or ('ID' in col)]\n",
    "    scale_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    scale_df = scale_df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    print(pg.cronbach_alpha(data=scale_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "net",
   "language": "python",
   "name": "net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
